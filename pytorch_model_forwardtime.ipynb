{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_model_forwardtime.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dTY8WSvulZo",
        "outputId": "b176369b-634a-44e5-b13a-3c3598d4a223"
      },
      "source": [
        "################################################################################\n",
        "# 1. time\n",
        "# https://discuss.pytorch.org/t/how-to-get-forward-time/25158\n",
        "\n",
        "# 2. start end\n",
        "# https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964\n",
        "################################################################################\n",
        "\n",
        "import cv2\n",
        "import onnx\n",
        "import torch\n",
        "# import time # forward1\n",
        "from albumentations import (Compose,Resize,)\n",
        "from albumentations.augmentations.transforms import Normalize\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    # transformations for the input data\n",
        "    transforms = Compose([\n",
        "        Resize(224, 224, interpolation=cv2.INTER_NEAREST),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # read input image\n",
        "    input_img = cv2.imread(img_path)\n",
        "    # do transformations\n",
        "    input_data = transforms(image=input_img)[\"image\"]\n",
        "    # prepare batch\n",
        "    batch_data = torch.unsqueeze(input_data, 0)\n",
        "\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "def postprocess(output_data):\n",
        "    # get class names\n",
        "    with open(\"imagenet_classes.txt\") as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "    # calculate human-readable value by softmax\n",
        "    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
        "    # find top predicted classes\n",
        "    _, indices = torch.sort(output_data, descending=True)\n",
        "    i = 0\n",
        "    # print the top classes predicted by the model\n",
        "    while confidences[indices[0][i]] > 0.5:\n",
        "        class_idx = indices[0][i]\n",
        "        print(\n",
        "            \"class:\",\n",
        "            classes[class_idx],\n",
        "            \", confidence:\",\n",
        "            confidences[class_idx].item(),\n",
        "            \"%, index:\",\n",
        "            class_idx.item(),\n",
        "        )\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    # load pre-trained model -------------------------------------------------------------------------------------------\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    # torch.cuda.synchronize() # forward1\n",
        "    # t0 = time.time() # forward1\n",
        "    mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "    # preprocessing stage ----------------------------------------------------------------------------------------------\n",
        "    input_batch = preprocess_image(\"/content/turkish_coffee.jpg\").cuda() # 경로 주의\n",
        "\n",
        "    # inference stage --------------------------------------------------------------------------------------------------\n",
        "    mobilenet_v2.eval()\n",
        "    mobilenet_v2.cuda()\n",
        "    output = mobilenet_v2(input_batch)\n",
        "    end.record()\n",
        "\n",
        "    # post-processing stage --------------------------------------------------------------------------------------------\n",
        "    torch.cuda.synchronize()\n",
        "    # t1=time.time() # forward1\n",
        "    # print(t1-t0) # forward1\n",
        "    print(\"model forward time : \" + str(start.elapsed_time(end) / 1000))\n",
        "    postprocess(output)\n",
        "\n",
        "    # convert to ONNX --------------------------------------------------------------------------------------------------\n",
        "    ONNX_FILE_PATH = \"mobilenet_v2.onnx\"\n",
        "    torch.onnx.export(mobilenet_v2, input_batch, ONNX_FILE_PATH, input_names=[\"input\"], output_names=[\"output\"], export_params=True)\n",
        "\n",
        "    onnx_model = onnx.load(ONNX_FILE_PATH)\n",
        "    # check that the model converted fine\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "\n",
        "    print(\"Model was successfully converted to ONNX format.\")\n",
        "    print(\"It was saved to\", ONNX_FILE_PATH)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model forward time : 2.0995625\n",
            "class: cup , confidence: 93.23870849609375 %, index: 968\n",
            "class: vase , confidence: 1.3668783903121948 %, index: 883\n",
            "class: pitcher, ewer , confidence: 1.2176575660705566 %, index: 725\n",
            "class: coffeepot , confidence: 1.2141094207763672 %, index: 505\n",
            "class: coffee mug , confidence: 0.8338580131530762 %, index: 504\n",
            "class: espresso , confidence: 0.5343857407569885 %, index: 967\n",
            "Model was successfully converted to ONNX format.\n",
            "It was saved to mobilenet_v2.onnx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT4GwQqq4CYx"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}